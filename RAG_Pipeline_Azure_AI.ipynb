{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install PyPDF2\n",
        "import PyPDF2\n",
        "\n",
        "\"\"\"\n",
        "Extracts text from PDF\n",
        "Parameters:\n",
        "    pdf_path (string): file path to pdf in directory tree\n",
        "Returns a string of the pdf's contents\n",
        "\"\"\"\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            text += reader.pages[page_num].extract_text()\n",
        "        return text\n",
        "\n",
        "pdf_path = \"/content/EU AI Act.pdf\"\n",
        "document_text = extract_text_from_pdf(pdf_path)\n",
        "print(document_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHI4Pt2OT2Fo",
        "outputId": "566c4c12-ea51-442a-bcfd-70bc4e4f6b8f",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "BRIEFING  \n",
            "EU Legislation in Progress  \n",
            " \n",
            "EPRS | European Parliamentary Research  Service  \n",
            "Author: Tambiama  Madiega  \n",
            "Members' Research Service \n",
            "PE 698.792  –  March 2024  EN \n",
            "Artificial intelligence act  \n",
            "OVERVIEW  \n",
            "European Union lawmakers reached a political agreement on the draft artificial intelligence (AI) act \n",
            "in December 2023. Proposed by the European Commission in April  2021, t he draft AI act, the first \n",
            "binding  worldwide  horizontal regulation  on AI, \n",
            "sets a common framework for the use and supply of \n",
            "AI systems in the EU. It offers  a classification for AI sy stems with different requirements and \n",
            "obligations tailored on a ' risk-based approach '. Some AI systems presenting 'unacceptable ' risks are \n",
            "prohibited. A wide range of 'high -risk' AI systems that can have a detrimental impact on people' s \n",
            "health, safety or on their fundamental rights  are authorised, but subject to a set of requirements and \n",
            "obligations to gain access to the EU market. AI systems posing limited risks because of their lack of \n",
            "transparency will be subject to information and transparency requirements, while AI systems \n",
            "presenting only minimal risk  for people will not be subject to further obligations. The regulation also \n",
            "provides specific rules for general purpose AI (GPAI) models  and lays down more stringent \n",
            "requirements for GPAI models with 'high -impact capabilities ' that could pose a systemic risk and \n",
            "have a significant impact on the internal market.  \n",
            "T\n",
            "he provisional agreement has been endorsed by the Committee of Permanent Representatives of \n",
            "EU Member States and by Parliament's two lead committee s. Parliament's plenary vote on the final \n",
            "agreement is scheduled for the March plenary session. The AI act must also be endorsed by Council \n",
            "and published in the EU 's Official Journal  before entering into force. \n",
            "Proposal for a regulation of the European Par liament and of the Council laying down harmonised \n",
            "rules on artificial intelligence (artificial intelligence act) and amending certain Union legislative \n",
            "acts  \n",
            "Committees \n",
            "responsible:  \n",
            " \n",
            "Rapporteurs:  \n",
            " \n",
            "Shadow rapporteurs:  Internal Market and Consumer Protection  (IMCO) and \n",
            "Civil Liberties, Justice and Home Affairs (LIBE) (jointly \n",
            "under Rule 58)  \n",
            "Brando Benifei (S&D, Italy)  and Dragoş Tudorache \n",
            "(Renew, Romania) \n",
            "Deirdre Clune, Axel Voss (EPP); Petar Vitanov (S&D); \n",
            "Svenja Hahn, (Renew); Sergey Lagodinsky, Kim  Van Sparrentak (Greens/EFA); Rob Rooken, \n",
            "Kosma Złotowski (ECR); Jean -Lin Lacapelle, Jaak \n",
            "Madison (ID); Cornelia Ernst, Kateřina Konecna (The \n",
            "Left)  COM(2021)206  \n",
            "21.4.2021  \n",
            "2021/0106(COD) \n",
            " \n",
            "Ordinary legislative \n",
            "procedure (COD) (Parliament and Council on equal footing  – formerly \n",
            "'co-decision ') \n",
            "Next steps expected:  Final first -reading vote in plenary  \n",
            " \n",
            "EPRS | European Parliamentary Research Service  \n",
            "2 Introduction  \n",
            "AI technologies are expected to bring a wide array of economic and societal benefits  to a wide \n",
            "range of sectors, including environment and health, the public sector, finance, mobility, home affairs \n",
            "and agriculture. They are particularly useful for improving prediction, for optimising operations and resource allocation, and for personalising services.\n",
            "1 However, the implications of AI systems for \n",
            "fundamental rights  protected under the EU Charter of Fundamental Rights, as well as the safety \n",
            "risks  for users when AI technologies are embedded in products and services, are raising concern. \n",
            "Most notably, AI systems may jeopardise fundamental rights such as the right to non -discrimination, \n",
            "freedom of expression, hu man dignity, personal data protection and privacy.2 \n",
            "Given the fast development of these technologies, in recent years AI regulation has become a \n",
            "central policy question in the European Union (EU). Policy -makers pledged  to develop a ' human -\n",
            "centric' approach to AI  to ensure that Europeans can benefit from new technologies developed \n",
            "and functioning according to the EU 's values and principles. In its 2020 White Paper on Artificial \n",
            "Intelligence, the European Commission committed to promote the uptake of  AI and address the \n",
            "risks associated with certain uses of this new technology. After having  initially adopted a soft -law \n",
            "approach  with the publication of its non -binding 2019 Ethics Guidelines for Trustworthy AI  and \n",
            "Policy and investment recommendations , the European Commission shifted  towards a legislative \n",
            "approach , calling for the adoption of harmonised rules  for the development, placing on the market \n",
            "and use of AI systems.  \n",
            "Leading the EU -level debate, the Parliament called on the Commission to assess the impact of AI \n",
            "and to draft an EU framework for AI, in its wide -ranging 2017 recommendations on civil law rules on \n",
            "robotics . In 2020 and 2021, Parliament adopted a number of non -legislative resolutions calling for \n",
            "EU action ,3 as well as two legislative resolutions asking the Commission to establish a legal \n",
            "framework of ethical principles  for the development, deployment and use of AI, robotics and related \n",
            "technologies in the Union and harmonising  the legal framework for civil liability  claims and \n",
            "imposition of a regime of strict liability on operators of high -risk AI systems.  \n",
            "In the past, the Council has repeatedly called for the adoption of common AI rules, including in 2017  \n",
            "and 2019. In 2020, the Council called  upon the Commission to put forward concrete proposals that \n",
            "take existing legislation into account and follow a risk -based, proportionate and, if necessary, \n",
            "regulatory approach.  \n",
            "The Commission launched a broad public consultation  in 2020 and published an  Impact Assessment \n",
            "of the regulation on artificial intelligence, a  supporting study  and a draft proposal , wh ich received \n",
            "feedback  from stakeholders.4 In its impact assessment, the Commission identifie d several problems  \n",
            "raised by the development and use of AI systems, due to their specific characteristics , namely:  \n",
            "(i) opacity (limited ability of the human mind to understand how certain AI systems operate), \n",
            "(ii) complexity, (iii)  continuous adaptation and unpredictability, (iv)  autonomous behaviour, and \n",
            "(v)  functional dependence on data and on the quality of data.  \n",
            "AI regulatory approach in the world . An increasing number of countries  worldwide are designing and \n",
            "implementing AI governance legislation and policies . While the United States of America (USA) had initially \n",
            "taken a lenient approach towards AI, calls  for regulation have recen tly been mounting. The White House has \n",
            "released the Blueprint for an AI Bill of Rights , a set of guidelines to protect the rights of the American public in \n",
            "the age of AI and President Joe Biden signed an executive order on AI  in 2023.  The Cyberspace A dministration \n",
            "of China issued some guidelines  on generative AI services, while the UK has announced  a pro -innovation \n",
            "approach to AI regulation, which largely regulates AI via existing laws. At international level, the Organisation \n",
            "for Economic Co -operation and Development (OECD) adopted some non -binding Principles on AI , in 2019, \n",
            "UNESCO embraced a set of Recommendations on the Ethics of AI  in 2021, the G7 agreed some International \n",
            "Guiding Principles on Art ificial Intelligence in 2023 and the Council of Europe is currently finalising an \n",
            "international convention on AI . Furthermore, in the context of the newly established EU -US tech partnership \n",
            "(the Trade and Technology Council), the EU and the USA are seeking to develop a mutual understanding on \n",
            "the principles underpinning trustworthy and responsible A I. Artificial intelligence act  \n",
            "3 The changes the proposal would bring  \n",
            "The draft AI act was  designed as a horizontal EU legislative instrument  applicable to all AI systems \n",
            "placed on the market or used in the Union, based on Article  114 and Article  16 of the Treaty on the \n",
            "Functioning of the European Union (TFEU)  following the logic of the new legislative framework  \n",
            "(NLF), i.e. the EU 's approach to ensuring a range of products comply with the applicable legislation \n",
            "when they are placed on the EU market through conformity assessments and the use of CE marking.   \n",
            "The Commission proposed enshrining  in EU law a legal  definition  of 'AI system ' referring  to a range \n",
            "of software -based technologies  using specific techniques and approaches (' machine learning' , \n",
            "'logic and knowledge -based ' systems, and ' statistical ' approaches ) that could be complemented  \n",
            "through the adoption of delegated acts  to facto r in technological developments .  \n",
            "The Commission also proposed to adopt a risk -based approach  whereby legal intervention was  \n",
            "tailored to concrete level of risk. Four categories were identified.  \n",
            "F\n",
            "irst, the draft act proposed to explicitly ban the following  harmful AI practices  that are considered \n",
            "to be a clear threat to people 's safety, livelihoods and rights, because of the ' unacceptable risk ' they \n",
            "create:  \n",
            " A\n",
            "I systems that deploy harmful manipulative 'subliminal techniques';  \n",
            " AI systems that exploit specific vulnerable groups (physical or mental disability);  \n",
            " AI systems used by public authorities, or on their behalf, for social scoring purposes;  \n",
            " 'Real -time ' remote biometric identification systems in publicly accessible spaces for \n",
            "law enforcement purposes, except in a limited number of cases.5 \n",
            "Second, the draft act proposed to regulate  high -risk  AI systems  that create adverse impact on \n",
            "people 's safety or their fundamental rights. The draft text distinguishe d between two categories of \n",
            "high -risk AI systems.  \n",
            " S\n",
            "ystems used as a safety component of a product or falling under EU health and safe ty \n",
            "harmonisation legislation  (e.g. toys, aviation, cars, medical devices, lifts).  \n",
            " Systems deployed in eight specific areas  specified in Annex (e.g. law enforcement),  \n",
            "whic h the Commission could update as necessary through delegated acts.  \n",
            "Such \n",
            "high- risk AI systems would have to comply with a range of requirements  particularly on risk \n",
            "management, testing, technical robustness, data training and data governance, transparency,  \n",
            "human oversight, and cybersecurity before being placed on the market or put  into service. AI \n",
            "systems that conform to new harmonised EU standards would benefit from a presumption of \n",
            "conformity with the draft AI act requirements. \n",
            "T\n",
            "hird, AI systems presentin g limited risk , such as systems that interacts with humans (i.e. chatbots), \n",
            "emotion recognition systems, biometric categorisation systems, and AI systems that generate or \n",
            "manipulate image, audio or video content (i.e. deepfakes), would be subject to a limi ted set of \n",
            "transparency obligations.  \n",
            "Finally, \n",
            "all other AI systems presenting only low or minimal risk  could be developed and used in \n",
            "the EU without conforming to any additional legal obligations. However, the proposed AI act \n",
            "envisage d the creation of codes of conduct  to encourage providers of non- high -risk AI systems to \n",
            "apply the mandatory requirements for high- risk AI systems voluntarily . \n",
            "The proposal required Member States to designate one or more competent authorities, including a \n",
            "national superviso ry authority, which would be tasked with supervising the application and \n",
            "implementation of the regulation, and  proposed to establish  a European Artificial Intelligence \n",
            "Board  (composed of representatives from the Member States and the Commission) at EU level. \n",
            "National market surveillance authorities  would be responsible for assessing operators ' \n",
            "compliance with the obligations and requirements for high- risk AI systems. Administrative \n",
            "fines  of EPRS | European Parliamentary Research Service  \n",
            "4 varying scales (up to €30 m illion or 6 % of the total worldwide ann ual turnover), depending on the \n",
            "severity of the infringement, were  set as sanctions for non -compliance with the AI a ct.  \n",
            "S\n",
            "ome measures were tailored to foster investments. The Commission propose d that Member States, \n",
            "or the European Data Protection Supervis or, could establish a regulatory sandbox , i.e. a controlled \n",
            "environment that facilitates the development, testing and validation of innovative AI systems (for a \n",
            "limited period of time) before they are put on the market. Sandboxing w ould  enable participants to \n",
            "use personal data to foster AI innovation, without prejudice to the GDPR  requirements. Other \n",
            "proposed measures were  tailored specifically to small -scale providers and start-ups .   \n",
            "Advisory c ommittees  \n",
            "The European Economic and Social Committee and the European Committee of the Regions \n",
            "adopted  their opinions in 2021 and in 2022, respectively.  \n",
            "National parliaments  \n",
            "The deadline for the submission of reasoned opinions  on the grounds of subsidiarity was \n",
            "2 September  2021. Contributions were received from the Czech Chamber of Deputies  and the Czech \n",
            "Senate , the Portuguese Parliament , the Polish Senate  and the German Bundesrat .  \n",
            "Stakeholder views6 \n",
            "Definitions  were  a contentious point of discussion among stakeholders. The Big Data Value \n",
            "Association, an industry -driven international not –for -profit organisation, stresse d that the definition \n",
            "of AI systems was  quite broad and would cover far more than what is subjectively understood as AI, \n",
            "including the simplest search, sorting and routing algorithms, which would consequently be subject \n",
            "to new rules. Further more, they asked  for clarification of how components of larger AI systems (such \n",
            "as pre -trained AI components from other manufacturers or components not released separately), \n",
            "should be treated. AmCham, the American Chamber of Commerce in the EU, suggest ed avoiding \n",
            "over -regulation by adopting a narrower definition of AI systems, focusing strictly on high -risk AI \n",
            "applications (and not extended to AI applications that are not high -risk, or software in general).  \n",
            "While they generally welcome d the proposed AI act' s risk -based approach, some stakeholders \n",
            "supported  wider prohibition and regulation of AI systems. Civil rights organisations call ed for a ban \n",
            "on indiscriminate or arbitrarily targeted use of biometrics in public or publicly accessible spaces, and for restrictions on the uses of AI systems, including for border control and predictive policing.  \n",
            "The European Enterprises Alliance stresse d that there was  general uncertainty about the roles and \n",
            "responsibilities of the different actors in the AI value chain (developers, providers, and users of AI systems). This was  particularly challenging for companies providing general purpose application \n",
            "programming interfaces or open -source AI models  \n",
            "t\n",
            "hat are not specifically intended for high- risk \n",
            "AI systems but are nevertheless used by third parties in a manner that could be considered high-risk. \n",
            "They also call ed for 'high -risk' to be redefined, based on the measurable harm and potential \n",
            "impact. AlgorithmWatch \n",
            "underlined  that the applicability of specific rules should not depend on the \n",
            "type of technology, but on the impact it has on individuals and society. They call ed for the new rules \n",
            "to be defined according to the impact of the AI systems and recommend that every operator should conduct an impact assessment that assesses the system 's risk levels on a case -by-case basis. Cli mate \n",
            "Change AI call ed for climate change mitigation and adaptation to be taken into account i n the \n",
            "classification rules for high -risk AI systems and impose environmental protection requirements.  \n",
            "The European Consumer Organisation, BEUC, stresse d that the proposal required substantial \n",
            "improvement to guarantee consumer protection . The organisation argue d that the proposal \n",
            "should have a broader scope and impose basic principles and obligations (e.g. on fairness, \n",
            "accountability and transparency) upon all AI systems, as well as prohibiting more comprehensively \n",
            "harmful practices (such as private entities ' use of social scoring and of remote biometric Artificial intelligence act  \n",
            "5 identification systems in public spaces). Furthermore, consumers should be granted a strong set of \n",
            "rights, effective remedies and redress mechanisms, including collective redress.  \n",
            "There were  opposing views on the impact of the proposed regulation on investment . A study  by \n",
            "the Centre for Data Innovation (representing large online platforms) highlighted  that the \n",
            "compliance costs incurred under the proposed AI act would likely provoke a chilling effect on investment in AI in Europe, and could particularly deter small and medium -sized enterpr ises (SMEs) \n",
            "from developing high- risk AI systems. According to the study , the AI act would cost the European \n",
            "economy €31  billion over the next five years and reduce AI investments by almost 20  %. However, \n",
            "such estimates of the compliance costs were  challenged by the experts  from the Centre for \n",
            "European Policy Studies, as well as by other ec onomists. The European Digital SME Alliance warned  \n",
            "against overly stringent conformity requirements, and asked  for effective SME representation in the \n",
            "standards- setting procedures and for mandatory sandboxes in all EU Member States.  \n",
            "Academic and other views  \n",
            "While generally supporting the Commission 's proposal, critics call ed for amendments, including \n",
            "revising the ' AI systems' defin ition, ensuring a better allocation of responsibility, strengthening \n",
            "enforcement mechanisms and fostering democratic participation.7 Among the main issues were:  \n",
            "AI systems definition  \n",
            "The legal definition of 'AI systems' contained in the proposed AI act has been heavily criticised . \n",
            "Smuha and others warned  the definition lacks clarity and may lead to legal uncertainty, especially \n",
            "for some systems that would not qualify as AI systems under the draft text, while their use may have \n",
            "an adverse impact on fundamental rights.8 To address this issue, the authors propose d to broaden \n",
            "the scope of the legislation  to include explicitly all computational systems used in the identified \n",
            "high -risk domains, regardless of whether they are considered to be AI. Ebers and others consider  \n",
            "that the scope of ' AI systems' was overly broad, which may lead to legal uncertainty  for developers, \n",
            "operators, and users of AI systems and ultimately to over -regulation.9 T hey c alled on EU law -makers \n",
            "to exempt AI systems developed and used for research purposes  and open -source software  (OSS) \n",
            "from regulation. Other commentators \n",
            "question ed whether the proposed definition of ' AI systems' \n",
            "is truly technology neutral  as it refers primarily to ' software ', omitting potential future AI \n",
            "developments. \n",
            "Risk -based approach  \n",
            "Academics also call ed for amendments, warning that the risk -based approach proposed by the \n",
            "Commission would not ensure a high level of protection of fundamental rights. Smuha and others \n",
            "argue d that the proposal \n",
            "does not always accurately recognise  the wrongs and harms associated \n",
            "with different kinds of AI systems and therefore does not appropriately allocate responsibility. \n",
            "Among other things, they \n",
            "recommend ed adding a proc edure that enables the Commission to \n",
            "broaden the list of prohibited AI systems, and propose d banning existing manipulative AI \n",
            "systems (e.g. deepfakes), social scoring and some biometrics. Ebers and others call ed for a more \n",
            "detailed classification of risks  to facilitate industry self -assessment and support, as well as \n",
            "prohibiting more AI systems  (e.g. biometrics), including in the context of private use . \n",
            "Furthermore, some highlight ed that the draft legislation did not address systemic sustainability \n",
            "risks  created by AI , especially in the area of climate and environmental protection.10  \n",
            "One of the major concerns raised was  that the rules on prohibited and high-risk practices might  \n",
            "p\n",
            "rove ineffective in practice, because the risk assessment was proposed to be  left to provider self -\n",
            "assessment. Veale and Zuiderveen Borgesius \n",
            "warn ed that most providers can arbitrarily classify \n",
            "most high -risk systems as adhering to the rules using self -assessment procedures alone. Smuha and \n",
            "others recommend ed exploring whether certain high-risk systems would not benefit from a \n",
            "conform ity assessment carried out by an independent entity  prior to their deployment.  EPRS | European Parliamentary Research Service  \n",
            "6 Biometrics regulation.  A study commissioned by the European Parliament recommended , inter alia, to \n",
            "empower the Commission to adapt the list of prohibited AI practices periodically, under the supervision of the \n",
            "European Parliament, and the adoption of a more comprehensive list of 'restricted AI applications' (comprising \n",
            "real-time remote biometric identification without limitation for law enforcement purposes). Regulation of \n",
            "facial recognition technologies (FRTs) was one of the most contentious issues.11 The European Data Protection \n",
            "Supervisor (EDPS) and the European Data Protec tion Board (EDPB) called  for a general ban on any uses of AI \n",
            "for the automated recognition of human features in publicly accessible spaces.  \n",
            "Governance structure and enforcement and redress mechanisms  \n",
            "Ebers et al.  stressed  that the AI act lacks effective enforcement structures , as the Commission \n",
            "propose d to leave the  preliminary risk assessment, including the qualification as high- risk, to the \n",
            "providers ' self-assessment. They also raise d concerns about the excessive delegation of regulatory \n",
            "power to private European standardisation organisations (ESOs), due to the lac k of democratic \n",
            "oversight, the impossibility for stakeholders (civil society organisations, consumer associations) to \n",
            "influence the development of standards, and the lack of judicial means to control them once they have been adopted. Instead, they recommen ded that the AI act codify  a set of legally binding \n",
            "requirements for high- risk AI systems, which ESOs may specify through harmonised standards.  \n",
            "Commentators regretted  a crucial gap in the AI act  – the lack of provision  provide for individual \n",
            "enforcement r ights . Ebers and others stressed  that individuals affected by AI systems and civil \n",
            "rights organisations have no right to complain  to market surveillance authorities or to sue a \n",
            "provider or user for failure to comply with the requirements. Similarly, Veale and Zuiderveen \n",
            "Borgesius warn ed that, while some provisions of the draft legislation aim to impose obligations on \n",
            "AI systems users, no mechanism for complaint or judicial redress  was available to them. Smuha \n",
            "and others recommend ed amending the proposal to include, inter alia, an explicit right of redress \n",
            "for individuals  and rights of consultation and participation for EU citizens  regarding the \n",
            "decision to amend the list of high- risk systems in Annex  III. \n",
            "It has also been stressed  that the text proposed by the Commission  lack ed proper coordination  \n",
            "mechanisms between authorities, in particular concerning cross -border infringement . \n",
            "Furthermore, guidance would be desirable  on how to ensure compliance with trans parency and \n",
            "information requirements, while simultaneously protecting intellectual property rights and \n",
            "trade secrets , not least to avoid diverging practices in the Member States.  \n",
            "Legislative process  \n",
            "The Council  adopted its common position  in December 2022. In Parliament , the file was assigned \n",
            "jointly (under Rule  58) to the Commit tee on Internal Market and Consumer Protection (IMCO) and \n",
            "the Committee on Civil Liberties, Justice and Home Affairs (LIBE), with Brando  Benifei (S&D, Italy) and \n",
            "Drago ş Tudorache, Renew, Romania) appointed as rapporteurs. Parliament adopted  its negotiating \n",
            "position (499  votes in favour, 28  against and 93  abstentions) in June  2023, with substantial \n",
            "amendments  to the Commission 's text.  Following protracted negotiations, the Council and the \n",
            "European Parliament  reached a provisional agreement  on the AI act on 9  December 2023. The \n",
            "European Parliament 's LIBE and IMCO committees endorsed  the final text  in a joint vote on \n",
            "13 February  2024, with an overwhelming majority (71  votes in favour, 8  votes against and \n",
            "7 abstentions).  The European Parliament will now vote on the final agreement on the AI act at the \n",
            "March  2024 plenary  session, before it is endorsed by Council ands published in the EU's Official \n",
            "Journal. The main points of the EU AI rules are:  \n",
            "Definitions  \n",
            "The AI act enshrines in EU law a definition  of AI systems  aligned with the revised definition agreed \n",
            "by the OECD :  Artificial intelligence act  \n",
            "7 'An AI system is a machine -based system designed to operate with varying levels of autonomy and \n",
            "that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, \n",
            "from the input it receives, how to generate outputs such as predictions, content, recommendations, \n",
            "or decisions that can influence physical or virtual environments '.  \n",
            "The definition is not intended to cover simpler traditional software sys tems or  programming \n",
            "approaches, and the Commission has been tasked to develop guidelines  on its application.    \n",
            "The act also contains a definition of g eneral purpose  artificial intelligence  (GPAI) models  'that \n",
            "are trained with a large amount of data using self -supervision at scale ', that display ' significant \n",
            "generality ' and are  'capable to competently perform a wide range of distinct tasks' and 'can be \n",
            "integrated into a variety of downstream systems or applications' . Furthermore, t\n",
            "he AI act defines \n",
            "general -purpose AI systems  as systems based on a GPAI model, which have the capability to serve \n",
            "a variety of purposes, both for direct use as well as for integration in other AI systems.   \n",
            "Scope of application   \n",
            "The AI act  applies  primarily to providers and deployers putting AI systems and GPAI models into \n",
            "service or placing  on the EU market and who have their place of establishment or who are located \n",
            "in the EU, as well as to deployers or providers of AI systems that are established in a third country, \n",
            "when the output produced by their systems is used in the EU .12 However, AI systems placed on the \n",
            "market, put into service, or used by public and private entit ies for military, defence or national \n",
            "security  purposes, are excluded from the scope. Similarly, the AI a ct will not apply to AI systems and \n",
            "models, including their output , which  are specifically developed and put into service for the sole \n",
            "purpose of scientific research and development . Furthermore, as matter of  principle, the \n",
            "regulation does not apply prior to the  systems and models being put into service or placed on the \n",
            "market  (sandboxing rules may apply in this case).  \n",
            "Risk -based approach \n",
            "EU AI act risk -based approach  \n",
            " \n",
            "Data source: European Commission  \n",
            "EPRS | European Parliamentary Research Service  \n",
            "8 The final agreement maintains the risk -based approach proposed by the Commission and classifies \n",
            "AI systems into several risk categories, with different degrees of regulation applying .  \n",
            " P\n",
            "rohibited AI practices . The final text prohibits a wider range of AI practices as \n",
            "originally proposed by the Commission because of their harmful impact:   \n",
            " AI systems using subliminal or manipulative or deceptive techniques  to \n",
            "distort people 's or a group of people's behaviour and impair informed \n",
            "decision -making, leading to significant harm;  \n",
            " AI systems exploiting vulnerabilities due to age, disability, or social or \n",
            "economic situations, causing significant harm;  \n",
            " Biometric categorisation systems inferring race, political opinions, trade \n",
            "union membership, religious or philosophical beliefs, sex life, or sexual orientation (except for lawful labelling or filtering in law -enforcement \n",
            "purpose s);  \n",
            " AI systems evaluating or classifying individuals or gr oups based on social \n",
            "behaviour or personal characteristics, leading to detrimental or disproportionate treatment in unrelated contexts or unjustified or disproportionate to their behaviour;  \n",
            " 'Real -time ' remote biometric identification in public spaces for law \n",
            "enforcement (except for specific necessary objectives such as searching for \n",
            "victims of abduction, sexual exploitation or missing persons, preventing \n",
            "certain substantial and imminent threats to safety, or identifying suspects in serious crimes);  \n",
            " AI systems assessing the risk of individuals committing criminal offences \n",
            "based solely on profiling or personality traits and characteristics (except when \n",
            "supporting human assessments based on objective, verifiable facts linked to a criminal activity);  \n",
            " AI system s creating  or expanding  facial recognition databases through \n",
            "untargeted scraping from the internet or CCTV footage;  \n",
            " AI systems inferring emotions in workplaces or educational institutions, \n",
            "except for medical or safety reasons.  \n",
            " High -risk AI systems. The A I act identifies  a number of use cases in which AI systems  \n",
            "are to be considered high  risk because they can  potenti ally create an adverse impact \n",
            "on people's health, safety or their fundamenta l rights.   \n",
            " The risk classification  is based on the intended purpos e of the AI system. The \n",
            "function performed by the AI system and the specific purpose and modalities \n",
            "for which the system is used are key to determine if an AI system is high -risk \n",
            "or not. High -risk AI systems can be safety components of products covered by \n",
            "sectoral EU law  (e.g. medical devices) or AI systems that, as a matter of  \n",
            "principle , are considered to be high- risk when they are used in specific areas  \n",
            "listed in an annex.13 The Commission is tasked with maintaining an EU \n",
            "database for  the high -risk AI  systems listed in this annex.  \n",
            " A new test has been enshrined at the Parliament's request (' filter provision '), \n",
            "according to which  AI systems will not be considered high -risk if they do not \n",
            "pose a significant risk of harm to the health, safety or fundament al rights of \n",
            "natural persons.14 However, an AI system will always be considered high- risk \n",
            "if the AI system performs profiling of natural persons.  \n",
            " Providers of such high -risk AI systems will have to run a conformity \n",
            "assessment  procedure  before their products  can be sold and used in the EU. \n",
            "They will need to comply with a range of requirements including for testing , \n",
            "data training  and  cybersecurity and , in some cases, will have to conduct a \n",
            "fundamental rights impact assessment to e nsure their systems compl y with \n",
            "EU law. The conformity assessment should be carried out either based on Artificial intelligence act  \n",
            "9 internal control (self -assessment) or with the involvement of a notified body \n",
            "(e.g. biometrics). C ompliance with European harmonised standard s to be \n",
            "developed  will grant high- risk AI  systems providers a presumption of \n",
            "conformity. After such AI systems are placed in the market, providers must \n",
            "implement p ost-market monitoring and take corrective actions if necessary. \n",
            " Transparency risk . Certain AI systems intended to interact with natural persons or to \n",
            "generate content may pose specific risks of impersonation or deception, irrespective \n",
            "of whether they qualify as high-risk AI systems or not. Such systems are subject to \n",
            "information and transparency requirements. Users must be made aware that they \n",
            "interact with chatbots. \n",
            "Deployers of AI systems that generate or manipulate image, \n",
            "audio or video content (i.e. deep fakes ), must disclose that the content has been \n",
            "artificially generated or manipulated except in very limited cases (e.g. when it is used \n",
            "to prevent criminal offences ). Providers of AI systems that generate large quantities of \n",
            "synthetic content  must implement sufficiently reliable, interoperable, effective and \n",
            "robust techniques and met hods (such as watermarks) to enable marking and \n",
            "detection that the output has been generated or manipulated by an AI system and \n",
            "not a human. Employers who deploy AI systems in the workplace  must inform the \n",
            "workers and their representatives.  \n",
            " M\n",
            "inimal risks . Systems presenting minimal risk for people (e.g. spam filters) will not \n",
            "be subject to further obligations beyond  current ly applicable legislation (e.g., GDPR).   \n",
            " General -purpose AI (GPAI) . The r egulation provides specific rules for general -\n",
            "purpose AI models and for general -purpose AI models that pose systemic risks.  \n",
            " GPAI system transparency requirements. All GPAI models will have to draw \n",
            "up and maintain  up-to-date technical documentation and make information \n",
            "and documentation available to downstream providers of AI systems. A ll \n",
            "providers of GPAI models have to put a policy in place to respect Union \n",
            "copyright law , including through state -of-the-art technologies (e.g. \n",
            "watermarking), to carry out lawful text- and -data mining exceptio ns as \n",
            "envisaged  under the Copyright D irective. Furthermore, GPAIs must draw up \n",
            "and make publicly available a sufficiently detailed summary of  the content \n",
            "used in training the GPAI models according to a te mplate provided by the \n",
            "AI Office .15 Finally, if located outside the EU, they will have to appoint a \n",
            "representative in the EU. However, AI models made accessible under a free \n",
            "and open source  will be exempt from some of the obligations (i.e. disclosure \n",
            "of tec hnical documentation) given they have, in principle, positive effects on \n",
            "resea rch, innovation and competition .16  \n",
            " Systemic -risk GPAI obligations . GPAI models with ' high -impact \n",
            "capabilities ' could pose a systemic risk and have a significant impact on the \n",
            "internal market, due to their reach and their actual or reasonably foreseeable negative effects (on public health, safety, public security, fundamental rights, or the society as a whole). GPAI providers must therefore notify the European \n",
            "Commission if their model is trained using a total computing power  \n",
            "exceeding 10 ^25 FLOPs (i.e. floating- point operations per second). When this \n",
            "threshold is met, the presumption will be that the model is a GPA I model \n",
            "posing  systemic risks.\n",
            "17 In addition to the requirements on transparency and \n",
            "copyright protection falling on all GPAI models, providers of systemic -risk \n",
            "GPAI models are required to constantly assess and mitigate  the risks  they \n",
            "pose and to ensure cybersecurity protection. That requires, inter alia, keep ing \n",
            "track of, document ing and report ing serious incidents (e.g. violations of \n",
            "fundamental rights) and implement ing corrective measures.  \n",
            " Code s of practice  and presumption of conformity. GPAI model  provi ders  \n",
            "will be able to rely on c odes of p ractice to demonstrate compliance with the EPRS | European Parliamentary Research Service  \n",
            "10 obligations  set under the act. By means of implementing acts, the \n",
            "Commission may decide to approve a code of practice and give it a general \n",
            "validity within the EU , or alternatively, provide common rules for \n",
            "implement ing the relevant obligations . Compliance with a European \n",
            "harmonised standard grants GPAI providers the presumption of conformity . \n",
            "Providers of GPAI models with systemic risks who do not adhere to an \n",
            "approved code of practice will be required to demonstrate adequate alternative means of compliance.  \n",
            "Sandboxing and real- world testing  \n",
            "The measures to support investment  in AI systems have been strengthened. National  authorities \n",
            "must establish at least one AI regulatory sandbox at national level to facilitate the development and \n",
            "testing of innovative AI systems under strict regulatory oversight.18 Such regulatory sandbox es \n",
            "provide for a controlled environment that fosters innovation and facilitates the developm ent, \n",
            "training, testing and validation of innovative AI systems for a limited time before their placement on the market or entry  into service . The AI regulatory sandbox must enable, where appropriate, t esting \n",
            "of AI systems in real -world conditions outside o f a laboratory for a limited period (subject to \n",
            "compliance with EU data protection law rules and principles). Furthermore, to accelerate the \n",
            "development and placing on the  market of high -risk AI systems, providers or prospective providers \n",
            "of such  systems may also test them in real -world conditions  – even without participating  in an AI \n",
            "regulatory sandbox  – if they respect some guarantees and conditions  (e.g. ask for specific consent, \n",
            "submit their real -world testing plan to the market surveillance a uthori ty).  \n",
            "Enforcement and institutional setting  \n",
            "The implementation of the act will be the responsibility of a number of national and EU -level actors. \n",
            "Member States must establish or designate at least one market surveillance authority and at least \n",
            "one notifying authority to ensure the application and implementation of the act. Heavy fines  will \n",
            "fall on non -compliant entities .19 At EU level , a range  of actors including the Commission, the AI \n",
            "Board, the AI O ffice, the EU standardisation bodies (CEN and CENELE C) and an advisory forum and \n",
            "scientific panel of independent experts  will support the implementation of the act. The EU AI Office  \n",
            "was established to provide advice on the implementation of the new rules , in particular as regards \n",
            "GPAI  models and to develop codes of practice to support the proper application of the AI a ct.   \n",
            "'Entry into force ' timelines  \n",
            "Prohibited systems have to be phased out within six months  after the act enters into force. The \n",
            "provisions concerning GPAI and penalties will apply 12 months  after the act enters into force , and \n",
            "those concerning high -risk AI systems apply 24 months  after entry into force (36 months after  entry \n",
            "into force  for AI systems covered by existing EU product legislation) . The c odes of practice envisaged \n",
            "must be ready , at the latest, nine months after the AI act enters into force. The implementation of \n",
            "the AI a ct requires a number of steps to be taken. In the coming months, t he Commission is expected \n",
            "to issue various implementing, delegated and guidelines  related to the a ct20 and to oversee the \n",
            "standardisation process  required for implementing the obligations .21 \n",
            "Policy debate latest issues . Academics have raised a number of questions as regard s the final text of the AI \n",
            "act and the implementation challenges lying ahead. Hacker welcomes the final AI act text but stresses, inter \n",
            "alia: that alignment with existing sectoral regulation is incomplete (which results in unnecessary and highly \n",
            "detrimental red tape); c ompliance costs will be substantial, especially for SMEs developing narrow AI models ; \n",
            "the threshold of 10^25 FLOPs for a default categori sation of systemic risk models is too high ; and calls for \n",
            "European supervision and monitoring of remote biometric identification to avoid the risk that some Member \n",
            "States  circumven t the rules enshrined in the AI act.22 Kutterer argues the AI act 's implementation will require \n",
            "a robust taxonomy setting out the correlation of risk classification and model capabilities and asses sing  the \n",
            "developments of open sources models.23 Helberger and others call for  the AI act to be complemented by  an \n",
            "additional set of exercisable rights to protect citizens from AI-generated harm, with additional legislation to Artificial intelligence act  \n",
            "11 control the potential environmental impact of training AI models and protect worker's rights and to define \n",
            "further a set of requirements that research organisations must comply with to benefit from the research \n",
            "exemption.24 Also, some argue  that the AI act d oes not go far enough in preventing and/or mitigating the \n",
            "specific risks associated with chatbots. T imely standardisation will be key to ensur ing adequate \n",
            "implementation of the AI act, for instance, to ensure the robustness of high -risk AI systems and the \n",
            "watermarking  of AI -generated content while, in the meantime, the EU is fostering the adoption of voluntary \n",
            "codes of conduct  and of an AI Pact  to mitigate the potential downsides of generative AI. Some academics \n",
            "warn  that that the standardisation and codification processes might not include representative groups of \n",
            "stakeholders and risks privileging regulated parties. Ensuring  international harmonisation  of AI governance \n",
            "has become a key topic for policymakers. More cooperation on aligning  AI governance between the EU and \n",
            "the US A is seen as crucial for AI's democratic governance.25 Key questions such as setting a common \n",
            "terminology  and addre ssing dual -use and military AI applications  have been raised in this respect. Finally, \n",
            "generative AI is seen as a disruptive technology that will likely mean amending EU laws and regulation , \n",
            "including in intellectual property rights , privacy and data protection and cyb ersecurity.  \n",
            "EUROPEAN PARLIAMENT SUPPORTING ANALYSIS  \n",
            "AI Reposito ry, EPRS, STOA Centre for Artificial Intelligence (C4AI), October 2023.  \n",
            "Biometric Recognition and Behavioural Detection , Policy Department for Citizens ' Rights and \n",
            "Constitutional Affairs, August 2021.  \n",
            "Dalli H., Artificial Intelligence Act: Initial Appraisal of the European Commission Impact Assessment , EPRS, \n",
            "July 2021.  \n",
            "Dumbrava C., Artificial intelligence at EU borders: Overview of applications and key issues , EPRS, \n",
            "July 2021.  \n",
            "Madiega T. A. and Mildebrath H. A., Regulating  facial recognition in the EU , EPRS, September 2021.  \n",
            "Madiega T., Artificial intelligence act and regulatory sandboxes , EPRS, March 2022.  \n",
            "Madiega T., General -purpose artificial intelligence, EPRS, March 2023.  \n",
            "Madiega T., Generative AI and watermarking , EPRS, December  2023.  \n",
            "OTHER SOURCES  \n",
            "Artificial Intelligence Act, European Parliament, Legislative Observatory (OEIL).  \n",
            "Novelli C. et al. , Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity , 2024.  \n",
            "Hacker P., Comments on the f inal trilogue version of the AI a ct, 2024.  \n",
            "ENDNOTES\n",
            "1  See European Commission, Proposal for a regulation of the European Parliament and of the Council laying down \n",
            "harmonised rules on artificial intelligence ( artificial intelligence act) 2021/0106 (COD) , Explanatory memorandum.  \n",
            "2  See for instance, High -Level Expert Group, Ethics Guidelines for Trustworthy AI , 2019.  \n",
            "3  See, inter alia, Recommendations on  intellectual property , criminal law , education, culture and audiovisual  areas, \n",
            "and regarding civil and military AI uses . \n",
            "4  For an overview s ee H. Dalli, Artificial intelligence act , above .  \n",
            "5  It was proposed to allow FRTs  (i) for targeted search for potential  victims of crime, including missing children ; (ii) to \n",
            "prevent a specific, substantial and imminent threat to the life or physical safety of persons or of a terrorist attack ; \n",
            "and (iii)  for the detection, localisation, identification or prosecution of a per petrator or individual suspected of a \n",
            "criminal offence referred to in the European Arrest Warrant Framework Decision . \n",
            "6  This section aims to provide a flavour of the de bate and is not intended to be an exhaustive account of all different \n",
            "views on the proposal. Additional information can be found in publications listed under 'supporting analysis'.  \n",
            "7  For an in -depth analysis of the proposals and recommendations for amendm ents see N. Smuha et al. , How the EU \n",
            "can achieve legally trustworthy AI: A response to the European Commission 's proposal for an a rtificial intelligence \n",
            "act, Elsevier, August 2021; M. Ebers, and others, The European Commission’s proposal for an a rtificial intelligence \n",
            "act—A critical assessment by members of the Robotics and AI Law Society (RAILS) , J 4, no  4: 589-603, October 2021.  \n",
            "8  N. Smuha, et al., above , at pp. 14 -15.;M. Veale and F.  Zuiderveen  Borgesius., Demystifying the draft EU AI a ct, 22(4) \n",
            "Computer Law Review International, Ju ly 2021.  \n",
            "9  See M. Ebers and others, above.  \n",
            "10  See V. Galaz and others, Artificial intelligence, systemic risks, and sustainability , Vol 67, Technology in Society , 2021.   EPRS | European Parliamentary Research Service  \n",
            "12  \n",
            "11  For an overview, see T. Madiega and H. Mildebrath, Regulating facial recognition in the EU, 2021.  \n",
            "12  The act applies to private organisations as well as to public authorities.  \n",
            "13  The Annex refers  to AI systems used in areas of  critical infrastructures (e.g.  road traffic ), education and vocational \n",
            "training , employment  worker management and access to self -employment , access to essential private and public \n",
            "services  and benefits  (e.g., creditworthiness evaluation ), law enforcement, border control, administration of justice \n",
            "and democratic processes , biometric identification, categorisation and emotion recognit ion systems (outside the \n",
            "prohibited categories) . \n",
            "14  An AI system will not be considered as high -risk if one or more of the following criteria are fulfilled: (i) the AI system \n",
            "is intended to perform a narrow procedural task; (ii) the AI system is intended t o improve the result of a previously \n",
            "completed human activity; (iii) the AI system is intended to detect decision -making patterns or deviations from prior \n",
            "decision -making patterns and is not meant to replace or influence the previously completed human assessment \n",
            "without proper human review; or (iv) the AI system is intended to perform a preparatory task to an assessment \n",
            "relevant for the purpose of the use cases listed in Annex III.  \n",
            "15  Established by European  Commissi on decision in January  2024 the AI Office  enter s into force in February  2024.  \n",
            "16   Furthermore , open -source models must comply with the AI act when they are integrated into prohibited AI \n",
            "practices or into high -risk systems and when they are considered to present systemic risk . \n",
            "17  FLOPs, or Floating -Point Operations Per Second, measur e a computer's processing speed. The threshold should be \n",
            "adjusted over time to reflect technological and industrial changes . Moreover, the Commission is entitled to  take \n",
            "individual decisions designating a GP AI mo del posing  systemic risk if it is found that such model has capabilities or \n",
            "impact equivalent to those captured by the FLOP threshold  on the basis of an overall assessment of criteria (e.g. \n",
            "quality or size of the training data set, number of business and end users, degree of autonomy and scalability ). In the \n",
            "USA, President  Biden's  AI executive order set 10^26 FLOPs as the threshold for AI models that need to be reported \n",
            "to the government with details of their training, capabilities and security.  \n",
            "18  Additional AI regulatory sandboxes at regional or local levels or jointly with other Member States' competent \n",
            "authorities may also be established . The European Data Protection Supervisor may also establish an AI regulatory \n",
            "sandbox for the EU institutions, bodies and agencies.  \n",
            "19  For instance, u p to €35 million  or 7 % of the total worldwide annual turnover of the preceding financial year \n",
            "(whichever is higher) for infringements  on prohibited practices or non -compliance  related to requirements on data .  \n",
            "20  Implementing acts must be adopted by the Commission to e stablish common specifications for req uirements for \n",
            "high -risk systems, to a pprove codes of practice on g enerated or manipulated content and to specify common rules \n",
            "for implementation if such codes of practice are deemed not adequate. Delegated acts will need to be adopted to \n",
            "identify c onditions for AI system s to not be  considered  high -risk and to s pecify and update criteria of GPAI posing  \n",
            "systemic risk , inter alia. The AI Office will have to draw up the codes of practice for GPAI providers . \n",
            "21  The Commission mandated the  European Standardisation Organisations  (CEN -CENELEC) to deliver a series of \n",
            "European standards to implement the AI act by January 2025.  \n",
            "22  See P. Hacker , Comments on the f inal trilogue version of the AI a ct, 2024.  \n",
            "23  See C. Kutterer, Regulati ng foundation models in the AI a ct: from \" high\" to \"systemic \" risk, 2024.  \n",
            "24  See N. Helberger and others, The Amsterdam Paper: Recommendations for th e technical finalisation of the \n",
            "regulation of GPAI in the AI a ct, 2024. See also , P. C havez, An AI c hallenge: Balancing o pen and c losed systems , 2023.  \n",
            "25 See A. Engler,  The EU and U.S. diverge on AI regulation: A transatlantic comparison and steps to alignment , 2023.  \n",
            " \n",
            "DISCLAIMER AND COPYR IGHT  \n",
            "This document is prepared for, and addressed to, the Members and staff of the European Parliament as \n",
            "background material to assist them in their parliamentary work. The content of the document is the sole \n",
            "responsibility of its author(s) and any opinions expressed herein should not be taken to represent an official \n",
            "position of the Parliament.  \n",
            "Reproduction and translation for non- commercial purposes are authorised, provided the source is \n",
            "acknowledged and the European Parliament is given prior notice and sent a copy.  \n",
            "© European Union, 202 4. \n",
            "eprs@ep.europa.eu  (contact)  \n",
            "www.eprs.ep.parl.union.eu  (intranet)  \n",
            "www.europarl.europa.eu/thinktank  (internet)  \n",
            "http://epthinktank.eu  (blog)  \n",
            "Third  edition. 'EU Legislation in Progress ' briefings are updated at key stages of the legislative procedure.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index\n",
        "!pip install pymilvus\n",
        "import os\n",
        "import numpy as np\n",
        "from openai import AzureOpenAI\n",
        "import textwrap\n",
        "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3ZsHNb9XPXt",
        "outputId": "458e3d63-bd6f-493a-d3df-3ee0ad7f7192",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.11.10)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.4)\n",
            "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.10 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.11.10)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.5)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.48.post3)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.9)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.1)\n",
            "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.0)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.2)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.4.0,>=0.3.1->llama-index) (1.47.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.10->llama-index) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (3.3)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (1.16.0)\n",
            "Requirement already satisfied: llama-cloud>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index) (0.0.17)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.1.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.3.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse>=0.3.0->llama-index) (0.5.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.10->llama-index) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama-index) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.1->llama-index) (0.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.10->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.10->llama-index) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.10->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.10->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.10->llama-index) (3.1.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.10->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.10->llama-index) (3.22.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.10->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.10->llama-index) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Requirement already satisfied: pymilvus in /usr/local/lib/python3.10/dist-packages (2.4.6)\n",
            "Requirement already satisfied: setuptools>69 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (71.0.4)\n",
            "Requirement already satisfied: grpcio>=1.49.1 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (1.64.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (3.20.3)\n",
            "Requirement already satisfied: environs<=9.5.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (9.5.0)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (5.10.0)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.1.4)\n",
            "Requirement already satisfied: milvus-lite<2.5.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.4.10)\n",
            "Requirement already satisfied: marshmallow>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from environs<=9.5.0->pymilvus) (3.22.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from environs<=9.5.0->pymilvus) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from milvus-lite<2.5.0,>=2.4.0->pymilvus) (4.66.5)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow>=3.0.0->environs<=9.5.0->pymilvus) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Azure Openai\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"4b3ea2d5074c4059ad670e016e83b853\"\n",
        "client = AzureOpenAI(\n",
        "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    api_version = \"2024-02-01\",\n",
        "    azure_endpoint = \"https://dxc-eu-ai-act-chatbot.openai.azure.com/\"\n",
        ")\n",
        "\n",
        "# model: gpt 35 turbo\n",
        "# model version: 0301\n",
        "deployment_name = \"DXC\"\n",
        "\n",
        "# Send a test completion call to generate an answer\n",
        "prompt = \"What is the EU AI Act?\"\n",
        "response = client.completions.create(\n",
        "    model = deployment_name,\n",
        "    prompt = prompt,\n",
        "    temperature = 1,\n",
        "    max_tokens = 100,\n",
        "    top_p = 0.5,\n",
        "    frequency_penalty = 0,\n",
        "    presence_penalty = 0,\n",
        "    stop = None\n",
        ")\n",
        "\n",
        "print(\"User prompt: \" + prompt)\n",
        "print(\"Response: \" + response.choices[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGUChfyXocaq",
        "outputId": "12e6dc83-8cb5-4f40-b448-754b39cd381f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User prompt: What is the EU AI Act?\n",
            "Response:  A Q&A with DLA Piper’s Giulio Coraggio\n",
            "\n",
            "The EU AI Act is a proposed set of regulations on the use of artificial intelligence in the European Union. The act is intended to establish a legal framework for AI in the EU and to ensure that AI is developed and used in a way that is safe, transparent, and ethical. Giulio Coraggio, a partner at DLA Piper, discusses the key provisions of the EU AI Act and what it means for businesses operating in the EU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Milvus\n",
        "ENDPOINT = \"https://in03-d592609349d65df.serverless.gcp-us-west1.cloud.zilliz.com\"\n",
        "TOKEN = \"c96536849f835f69648e8e1586f3e09794e9d9c63c2375d55bb62aa5a7b031bba566b53d6afd4410bb85f4aeea14f1406b947fe5\"\n",
        "connections.connect(\n",
        "   uri = ENDPOINT,\n",
        "   token = TOKEN)\n",
        "\n",
        "# Define schema for Milvus collection\n",
        "fields = [\n",
        "    FieldSchema(name = \"id\", dtype = DataType.INT64, is_primary = True),\n",
        "    FieldSchema(name = \"embedding\", dtype = DataType.FLOAT_VECTOR, dim = 1536),\n",
        "    FieldSchema(name = \"text\", dtype = DataType.VARCHAR, max_length = 65535)\n",
        "]\n",
        "\n",
        "schema = CollectionSchema(fields, \"EU AI Act Collection\")\n",
        "collection = Collection(\"eu_ai_act\", schema)\n",
        "\n",
        "\"\"\"\n",
        "  Generates embeddings using Azure OpenAI\n",
        "  Parameters:\n",
        "      text (string): section of text to create embeddings for\n",
        "  Returns embedding of the text, vector of floats\n",
        "\"\"\"\n",
        "def generate_embedding(text):\n",
        "    response = client.embeddings.create(\n",
        "        input = text,\n",
        "        model = \"DXC-embedding\"\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# Split the text into smaller chunks (vector of strings with max length of 5000 characters)\n",
        "chunk_size = 5000\n",
        "chunks = textwrap.wrap(document_text, chunk_size)\n",
        "print(\"Example chunk: \" + chunks[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2VhnKL5XQdS",
        "outputId": "345af9e6-1400-479f-dc23-207e72dbc829"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example chunk: BRIEFING   EU Legislation in Progress     EPRS | European Parliamentary Research  Service   Author: Tambiama  Madiega   Members' Research Service  PE 698.792  –  March 2024  EN  Artificial intelligence act   OVERVIEW   European Union lawmakers reached a political agreement on the draft artificial intelligence (AI) act  in December 2023. Proposed by the European Commission in April  2021, t he draft AI act, the first  binding  worldwide  horizontal regulation  on AI,  sets a common framework for the use and supply of  AI systems in the EU. It offers  a classification for AI sy stems with different requirements and  obligations tailored on a ' risk-based approach '. Some AI systems presenting 'unacceptable ' risks are  prohibited. A wide range of 'high -risk' AI systems that can have a detrimental impact on people' s  health, safety or on their fundamental rights  are authorised, but subject to a set of requirements and  obligations to gain access to the EU market. AI systems posing limited risks because of their lack of  transparency will be subject to information and transparency requirements, while AI systems  presenting only minimal risk  for people will not be subject to further obligations. The regulation also  provides specific rules for general purpose AI (GPAI) models  and lays down more stringent  requirements for GPAI models with 'high -impact capabilities ' that could pose a systemic risk and  have a significant impact on the internal market.   T he provisional agreement has been endorsed by the Committee of Permanent Representatives of  EU Member States and by Parliament's two lead committee s. Parliament's plenary vote on the final  agreement is scheduled for the March plenary session. The AI act must also be endorsed by Council  and published in the EU 's Official Journal  before entering into force.  Proposal for a regulation of the European Par liament and of the Council laying down harmonised  rules on artificial intelligence (artificial intelligence act) and amending certain Union legislative  acts   Committees  responsible:     Rapporteurs:     Shadow rapporteurs:  Internal Market and Consumer Protection  (IMCO) and  Civil Liberties, Justice and Home Affairs (LIBE) (jointly  under Rule 58)   Brando Benifei (S&D, Italy)  and Dragoş Tudorache  (Renew, Romania)  Deirdre Clune, Axel Voss (EPP); Petar Vitanov (S&D);  Svenja Hahn, (Renew); Sergey Lagodinsky, Kim  Van Sparrentak (Greens/EFA); Rob Rooken,  Kosma Złotowski (ECR); Jean -Lin Lacapelle, Jaak  Madison (ID); Cornelia Ernst, Kateřina Konecna (The  Left)  COM(2021)206   21.4.2021   2021/0106(COD)    Ordinary legislative  procedure (COD) (Parliament and Council on equal footing  – formerly  'co-decision ')  Next steps expected:  Final first -reading vote in plenary     EPRS | European Parliamentary Research Service   2 Introduction   AI technologies are expected to bring a wide array of economic and societal benefits  to a wide  range of sectors, including environment and health, the public sector, finance, mobility, home affairs  and agriculture. They are particularly useful for improving prediction, for optimising operations and resource allocation, and for personalising services. 1 However, the implications of AI systems for  fundamental rights  protected under the EU Charter of Fundamental Rights, as well as the safety  risks  for users when AI technologies are embedded in products and services, are raising concern.  Most notably, AI systems may jeopardise fundamental rights such as the right to non -discrimination,  freedom of expression, hu man dignity, personal data protection and privacy.2  Given the fast development of these technologies, in recent years AI regulation has become a  central policy question in the European Union (EU). Policy -makers pledged  to develop a ' human - centric' approach to AI  to ensure that Europeans can benefit from new technologies developed  and functioning according to the EU 's values and principles. In its 2020 White Paper on Artificial  Intelligence, the European Commission committed to promote the uptake of  AI and address the  risks associated with certain uses of this new technology. After having  initially adopted a soft -law  approach  with the publication of its non -binding 2019 Ethics Guidelines for Trustworthy AI  and  Policy and investment recommendations , the European Commission shifted  towards a legislative  approach , calling for the adoption of harmonised rules  for the development, placing on the market  and use of AI systems.   Leading the EU -level debate, the Parliament called on the Commission to assess the impact of AI  and to draft an EU framework for AI, in its wide -ranging 2017 recommendations on civil law rules on  robotics . In 2020 and 2021, Parliament adopted a number of non -legislative resolutions calling for  EU action ,3 as well as two legislative resolutions asking the Commission to establish a legal  framework of ethical principles  for the development,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for insertion into Milvus\n",
        "embeddings = np.array([generate_embedding(chunk) for chunk in chunks], dtype = np.float32)\n",
        "ids = list(range(len(embeddings)))\n",
        "data = [ids, embeddings, chunks]\n",
        "\n",
        "# Insert into Milvus\n",
        "collection.insert(data)\n",
        "collection.flush()\n",
        "print(\"Embeddings inserted into Milvus\")"
      ],
      "metadata": {
        "id": "7PZeyzOvzqtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_params = {\n",
        "    \"index_type\": \"IVF_FLAT\",\n",
        "    \"params\": {\"nlist\": 128},\n",
        "    \"metric_type\": \"L2\"\n",
        "}\n",
        "\n",
        "# Create an index on the embedding field in Milvus\n",
        "collection.create_index(field_name = \"embedding\", index_params = index_params)\n",
        "\n",
        "\"\"\"\n",
        "  Perform a similarity search based on user query\n",
        "  Parameters:\n",
        "      query_embeddings (numpy array): user's query embedding\n",
        "      top_k (int): max number of similar text chunks to return\n",
        "  Returns list (strings) of similar text chunks from pdf\n",
        "\"\"\"\n",
        "def query_milvus(query_embedding, top_k = 3):\n",
        "    collection.load()\n",
        "    # if not collection.has_index():\n",
        "    #     collection.create_index(\"embedding\", index_params)\n",
        "\n",
        "    results = collection.search(# Gathers list of metadata of the top sources\n",
        "        data = query_embedding,\n",
        "        anns_field = \"embedding\",\n",
        "        param = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 15}},\n",
        "        limit = top_k,\n",
        "        output_fields=[\"text\"]\n",
        "    )\n",
        "    print(results)\n",
        "    retrieved_chunks = [hit.entity.get(\"text\") for hit in results[0]]\n",
        "    return retrieved_chunks\n",
        "\n",
        "\n",
        "user_query = \"What are the rules around deep fakes?\"\n",
        "query_embedding = np.array([generate_embedding(user_query)], dtype = np.float32)\n",
        "relevant_text_chunks = query_milvus(query_embedding, 3)\n",
        "context = \" \".join(relevant_text_chunks)\n",
        "prompt = f\"Answer the question based on the following context: {context}\\n\\n{user_query}\"\n",
        "\n",
        "# Print metadata of the top retrieved text chunks from Milvus\n",
        "print(user_query + \"\\n\")\n",
        "print(\"Top Sources:\")\n",
        "num = 1\n",
        "for chunk in relevant_text_chunks:\n",
        "    print(\"   \" + str(num) + \". \" + chunk[:200])\n",
        "    num += 1\n",
        "\n",
        "response = client.completions.create(\n",
        "    model = deployment_name,\n",
        "    prompt = prompt,\n",
        "    temperature = 1,\n",
        "    max_tokens = 100,\n",
        "    top_p = 0.5,\n",
        "    frequency_penalty = 0,\n",
        "    presence_penalty = 0,\n",
        "    stop = None\n",
        ")\n",
        "print(\"\\nResponse: \" + response.choices[0].text)\n"
      ],
      "metadata": {
        "id": "m292G81QG4JJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38db643-4b0c-4674-e25c-030f7c660d7f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data: ['[\\'id: 7, distance: 0.4337833523750305, entity: {\\\\\\'text\\\\\\': \"do not adhere to an  approved code of practice will be required to demonstrate adequate alternative means of compliance.   Sandboxing and real- world testing   The measures to support investment  in AI systems have been strengthened. National  authorities  must establish at least one AI regulatory sandbox at national level to facilitate the development and  testing of innovative AI systems under strict regulatory oversight.18 Such regulatory sandbox es  provide for a controlled environment that fosters innovation and facilitates the developm ent,  training, testing and validation of innovative AI systems for a limited time before their placement on the market or entry  into service . The AI regulatory sandbox must enable, where appropriate, t esting  of AI systems in real -world conditions outside o f a laboratory for a limited period (subject to  compliance with EU data protection law rules and principles). Furthermore, to accelerate the  development and placing on the  market of high -risk AI systems, providers or prospective providers  of such  systems may also test them in real -world conditions  – even without participating  in an AI  regulatory sandbox  – if they respect some guarantees and conditions  (e.g. ask for specific consent,  submit their real -world testing plan to the market surveillance a uthori ty).   Enforcement and institutional setting   The implementation of the act will be the responsibility of a number of national and EU -level actors.  Member States must establish or designate at least one market surveillance authority and at least  one notifying authority to ensure the application and implementation of the act. Heavy fines  will  fall on non -compliant entities .19 At EU level , a range  of actors including the Commission, the AI  Board, the AI O ffice, the EU standardisation bodies (CEN and CENELE C) and an advisory forum and  scientific panel of independent experts  will support the implementation of the act. The EU AI Office   was established to provide advice on the implementation of the new rules , in particular as regards  GPAI  models and to develop codes of practice to support the proper application of the AI a ct.    \\\\\\'Entry into force \\\\\\' timelines   Prohibited systems have to be phased out within six months  after the act enters into force. The  provisions concerning GPAI and penalties will apply 12 months  after the act enters into force , and  those concerning high -risk AI systems apply 24 months  after entry into force (36 months after  entry  into force  for AI systems covered by existing EU product legislation) . The c odes of practice envisaged  must be ready , at the latest, nine months after the AI act enters into force. The implementation of  the AI a ct requires a number of steps to be taken. In the coming months, t he Commission is expected  to issue various implementing, delegated and guidelines  related to the a ct20 and to oversee the  standardisation process  required for implementing the obligations .21  Policy debate latest issues . Academics have raised a number of questions as regard s the final text of the AI  act and the implementation challenges lying ahead. Hacker welcomes the final AI act text but stresses, inter  alia: that alignment with existing sectoral regulation is incomplete (which results in unnecessary and highly  detrimental red tape); c ompliance costs will be substantial, especially for SMEs developing narrow AI models ;  the threshold of 10^25 FLOPs for a default categori sation of systemic risk models is too high ; and calls for  European supervision and monitoring of remote biometric identification to avoid the risk that some Member  States  circumven t the rules enshrined in the AI act.22 Kutterer argues the AI act \\\\\\'s implementation will require  a robust taxonomy setting out the correlation of risk classification and model capabilities and asses sing  the  developments of open sources models.23 Helberger and others call for  the AI act to be complemented by  an  additional set of exercisable rights to protect citizens from AI-generated harm, with additional legislation to Artificial intelligence act   11 control the potential environmental impact of training AI models and protect worker\\\\\\'s rights and to define  further a set of requirements that research organisations must comply with to benefit from the research  exemption.24 Also, some argue  that the AI act d oes not go far enough in preventing and/or mitigating the  specific risks associated with chatbots. T imely standardisation will be key to ensur ing adequate  implementation of the AI act, for instance, to ensure the robustness of high -risk AI systems and the  watermarking  of AI -generated content while, in the meantime, the EU is fostering the adoption of voluntary  codes of conduct  and of an AI Pact  to mitigate the potential downsides of generative AI. Some academics  warn  that that the standardisation and codification processes might not include representative groups\"}\\']'], cost: 6\n",
            "What are the rules around deep fakes?\n",
            "\n",
            "Top Sources:\n",
            "   1. do not adhere to an  approved code of practice will be required to demonstrate adequate alternative means of compliance.   Sandboxing and real- world testing   The measures to support investment  in A\n",
            "\n",
            "Response:  \n",
            "\n",
            "Deep fakes are a type of synthetic media that are created or altered by using deep learning or other machine learning algorithms to generate a likeness of something that never actually happened. The most common use of deep fakes is to create realistic looking fake videos of people doing or saying things that they never actually did or said. The problem with deep fakes is that they can be used to spread disinformation or to manipulate public opinion. As a result, many countries have started to regulate the use of deep\n"
          ]
        }
      ]
    }
  ]
}