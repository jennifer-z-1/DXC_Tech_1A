{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all needed packages and dependencies\n",
    "! pip install cohere -q\n",
    "! pip install cohere hnswlib unstructured -q\n",
    "!pip install pdfplumber\n",
    "!pip install --quiet langchain langchain_cohere langchain_experimental\n",
    "!pip install langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd90169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all needed packages and dependencies\n",
    "import cohere\n",
    "import pdfplumber\n",
    "import uuid\n",
    "import hnswlib\n",
    "import faiss\n",
    "from typing import List, Dict\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "import os\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "import random\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_cohere.react_multi_hop.agent import create_cohere_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_cohere.chat_models import ChatCohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc6075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Cohere API\n",
    "api_key = \"\"\n",
    "co = cohere.Client(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a32c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the PDF file and extract text as string\n",
    "file_path = \"\"\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    text = ''\n",
    "    # iterate through each page\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text() + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines structure for text\n",
    "lines = text.splitlines()\n",
    "title = \"\\n\".join(lines[:3])\n",
    "text = \"\\n\".join(lines[3:])\n",
    "\n",
    "documents = [\n",
    "    {\n",
    "        \"title\": title,\n",
    "        \"text\": text\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1260f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorstore:\n",
    "    \"\"\"\n",
    "    A class representing a collection of documents indexed into a vectorstore.\n",
    "\n",
    "    Parameters:\n",
    "    raw_documents (list): A list of dictionaries representing the sources of the raw documents. Each dictionary should have 'title' and 'url' keys.\n",
    "\n",
    "    Attributes:\n",
    "    raw_documents (list): A list of dictionaries representing the raw documents.\n",
    "    docs (list): A list of dictionaries representing the chunked documents, with 'title', 'text', and 'url' keys.\n",
    "    docs_embs (list): A list of the associated embeddings for the document chunks.\n",
    "    docs_len (int): The number of document chunks in the collection.\n",
    "    idx (hnswlib.Index): The index used for document retrieval.\n",
    "\n",
    "    Methods:\n",
    "    load_and_chunk(): Loads the data from the sources and partitions the HTML content into chunks.\n",
    "    embed(): Embeds the document chunks using the Cohere API.\n",
    "    index(): Indexes the document chunks for efficient retrieval.\n",
    "    retrieve(): Retrieves document chunks based on the given query.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents: List[Dict[str, str]]):\n",
    "        self.documents = documents\n",
    "        self.docs = []\n",
    "        self.docs_embs = []\n",
    "        self.retrieve_top_k = 10\n",
    "        self.rerank_top_k = 3\n",
    "        self.load_and_chunk()\n",
    "        self.embed()\n",
    "        self.index()\n",
    "\n",
    "\n",
    "    def load_and_chunk(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the pre-extracted text documents and stores them as chunks.\n",
    "        \"\"\"\n",
    "        print(\"Loading documents...\")\n",
    "\n",
    "        for document in self.documents:\n",
    "            title = document[\"title\"]\n",
    "            text = document[\"text\"]\n",
    "\n",
    "            # Assuming you want to chunk by paragraphs or a similar method\n",
    "            chunks = self.chunk_text(text)\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                self.docs.append(\n",
    "                    {\n",
    "                        \"title\": title,\n",
    "                        \"text\": str(chunk)\n",
    "                    }\n",
    "                )\n",
    "        print(f\"Loaded {len(self.docs)} document chunks.\")\n",
    "\n",
    "    def chunk_text(self, text: str, max_chunk_size: int = 500) -> list:\n",
    "        \"\"\"\n",
    "        Splits the text into chunks of a maximum size.\n",
    "        \"\"\"\n",
    "        # You can implement a more sophisticated chunking logic here\n",
    "        return [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
    "\n",
    "\n",
    "    def embed(self) -> None:\n",
    "        \"\"\"\n",
    "        Embeds the document chunks using the Cohere API.\n",
    "        \"\"\"\n",
    "        print(\"Embedding document chunks...\")\n",
    "\n",
    "        batch_size = 90\n",
    "        self.docs_len = len(self.docs)\n",
    "        for i in range(0, self.docs_len, batch_size):\n",
    "            batch = self.docs[i : min(i + batch_size, self.docs_len)]\n",
    "            texts = [item[\"text\"] for item in batch]\n",
    "            docs_embs_batch = co.embed(\n",
    "                texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\"\n",
    "            ).embeddings\n",
    "            self.docs_embs.extend(docs_embs_batch)\n",
    "\n",
    "    def index(self) -> None:\n",
    "        \"\"\"\n",
    "        Indexes the document chunks for efficient retrieval.\n",
    "        \"\"\"\n",
    "        print(\"Indexing document chunks...\")\n",
    "\n",
    "        self.idx = hnswlib.Index(space=\"ip\", dim=1024)\n",
    "        self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n",
    "        self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n",
    "\n",
    "        print(f\"Indexing complete with {self.idx.get_current_count()} document chunks.\")\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Retrieves document chunks based on the given query.\n",
    "\n",
    "        Parameters:\n",
    "        query (str): The query to retrieve document chunks for.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with 'title', 'text', and 'url' keys.\n",
    "        \"\"\"\n",
    "\n",
    "        # Dense retrieval\n",
    "        query_emb = co.embed(\n",
    "            texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\"\n",
    "        ).embeddings\n",
    "        \n",
    "        doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
    "\n",
    "        # Reranking\n",
    "        rank_fields = [\"title\", \"text\"] # We'll use the title and text fields for reranking\n",
    "\n",
    "        docs_to_rerank = [self.docs[doc_id] for doc_id in doc_ids]\n",
    "        rerank_results = co.rerank(\n",
    "            query=query,\n",
    "            documents=docs_to_rerank,\n",
    "            top_n=self.rerank_top_k,\n",
    "            model=\"rerank-english-v3.0\",\n",
    "            rank_fields=rank_fields\n",
    "        )\n",
    "\n",
    "        doc_ids_reranked = [doc_ids[result.index] for result in rerank_results.results]\n",
    "\n",
    "        docs_retrieved = []\n",
    "        for doc_id in doc_ids_reranked:\n",
    "            docs_retrieved.append(\n",
    "                {\n",
    "                    \"title\": self.docs[doc_id][\"title\"],\n",
    "                    \"text\": self.docs[doc_id][\"text\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return docs_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79181af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Vectorstore class with the given sources\n",
    "vectorstore = Vectorstore(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatbot(message, chat_history=None):\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "    \n",
    "    # Generate search queries, if any        \n",
    "    response = co.chat(message=message,\n",
    "                        model=\"command-r-plus\",\n",
    "                        search_queries_only=True,\n",
    "                        chat_history=chat_history)\n",
    "    \n",
    "    search_queries = []\n",
    "    for query in response.search_queries:\n",
    "        search_queries.append(query.text)\n",
    "\n",
    "    # If there are search queries, retrieve the documents\n",
    "    if search_queries:\n",
    "        print(\"Retrieving information...\", end=\"\")\n",
    "\n",
    "        # Retrieve document chunks for each query\n",
    "        documents = []\n",
    "        for query in search_queries:\n",
    "            documents.extend(vectorstore.retrieve(query))\n",
    "\n",
    "        # Use document chunks to respond\n",
    "        response = co.chat_stream(\n",
    "            message=message,\n",
    "            model=\"command-r-plus\",\n",
    "            documents=documents,\n",
    "            chat_history=chat_history,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        response = co.chat_stream(\n",
    "            message=message,\n",
    "            model=\"command-r-plus\",\n",
    "            chat_history=chat_history,\n",
    "        )\n",
    "        \n",
    "    # Print the chatbot response, citations, and documents\n",
    "    chatbot_response = \"\"\n",
    "    print(\"\\nChatbot:\")\n",
    "\n",
    "    for event in response:\n",
    "        if event.event_type == \"text-generation\":\n",
    "            print(event.text, end=\"\")\n",
    "            chatbot_response += event.text\n",
    "        if event.event_type == \"stream-end\":\n",
    "            if event.response.citations:\n",
    "                print(\"\\n\\nCITATIONS:\")\n",
    "                for citation in event.response.citations:\n",
    "                    print(citation)\n",
    "            if event.response.documents:\n",
    "                print(\"\\nCITED DOCUMENTS:\")\n",
    "                for document in event.response.documents:\n",
    "                    print(document)\n",
    "            # Update the chat history for the next turn\n",
    "            chat_history = event.response.chat_history\n",
    "\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates web search tool\n",
    "\n",
    "# connect to Tavily API\n",
    "tavily_api_key = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n",
    "\n",
    "# creates TavilySearchResults object\n",
    "internet_search = TavilySearchResults()\n",
    "internet_search.name = \"internet_search\"\n",
    "internet_search.description = \"Returns a list of relevant document snippets for a textual query retrieved from the internet.\"\n",
    "\n",
    "# creates web search tool\n",
    "class TavilySearchInput(BaseModel):\n",
    "    \"\"\"\n",
    "    A class inherits from BaseModel.\n",
    "\n",
    "    Attributes:\n",
    "    query (str):  a string representing the internet query to be searched\n",
    "    \"\"\"\n",
    "    query: str = Field(description=\"Query to search the internet with\")\n",
    "        \n",
    "# ensure format of input matches schema defined by TavilySearchInput class\n",
    "internet_search.args_schema = TavilySearchInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdac97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates python interpreter tool\n",
    "\n",
    "# create a new PythonREPL object\n",
    "python_repl = PythonREPL()\n",
    "python_tool = Tool(\n",
    "   name=\"python_repl\",\n",
    "   description=\"Executes python code and returns the result. The code runs in astatic sandbox without interactive mode, so print output or save output to a file.\",\n",
    "   func=python_repl.run,\n",
    ")\n",
    "python_tool.name = \"python_interpreter\"\n",
    "\n",
    "class ToolInput(BaseModel):\n",
    "    \"\"\"\n",
    "    A class that describes the input for PythonREPL object.\n",
    "    \n",
    "    Attributes:\n",
    "    code (str): a string containing the python code to execute\n",
    "    \"\"\"\n",
    "    code: str = Field(description=\"Python code to execute.\")\n",
    "\n",
    "# ensure format of input is valid\n",
    "python_tool.args_schema = ToolInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0528742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a random operation tool\n",
    "\n",
    "@tool\n",
    "def random_operation_tool(a: int, b: int):\n",
    " \"\"\"Calculates a random operation between the inputs.\"\"\"\n",
    " coin_toss = random.uniform(0, 1)\n",
    " if coin_toss > 0.5:\n",
    "   return {'output': a*b}\n",
    " else:\n",
    "   return {'output': a+b}\n",
    "\n",
    "random_operation_tool.name = \"random_operation\" # use python case\n",
    "random_operation_tool.description = \"Calculates a random operation between the inputs.\"\n",
    "\n",
    "class random_operation_inputs(BaseModel):\n",
    "    \"\"\"\n",
    "    A class that defines the input for the random operation tool.\n",
    "    \n",
    "    Attributes:\n",
    "    a (int):first input\n",
    "    b (int): second input\n",
    "    \"\"\"\n",
    "    a: int = Field(description=\"First input\")\n",
    "    b: int = Field(description=\"Second input\")\n",
    "        \n",
    "# validates the format of the inputs\n",
    "random_operation_tool.args_schema = random_operation_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom search tool that interacts with your vector store\n",
    "class VectorstoreSearchTool:\n",
    "    def __init__(self, vectorstore):\n",
    "        self.vectorstore = vectorstore\n",
    "\n",
    "    def search(self, query: str):\n",
    "        # Call the retrieve method from your Vectorstore instance\n",
    "        return self.vectorstore.retrieve(query)\n",
    "\n",
    "# Instantiate the search tool\n",
    "vectorstore_search = VectorstoreSearchTool(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f104533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide context to the LLM about our role\n",
    "context = \"We are a technology consulting company with global clients. It is important for our company and the clients of our company to stay within regulations of the strictest AI act, the EU AI Act. \"\n",
    "\n",
    "# pass case scenario to the LLM with context\n",
    "user_input = \"A client in the healthcare industry has approached our tech consulting company with a proposal for an AI doctor that can use a patient's information to detect health risk and diagnoses. How much risk does this project have according to the EU AI Act? Please provide quotes and citations from the document.\"\n",
    "message = context + user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fd2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn # 1\n",
    "chat_history = run_chatbot(\"Hello, I have a question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn # 2\n",
    "chat_history = run_chatbot(message, chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
